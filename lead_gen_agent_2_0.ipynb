{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium openai beautifulsoup4 pandas firecrawl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YzKS7oQWrua5",
        "outputId": "41124068-40d0-49a0-d587-75fbd4237cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.77.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting firecrawl\n",
            "  Downloading firecrawl-2.5.4-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from firecrawl) (2.32.3)\n",
            "Collecting python-dotenv (from firecrawl)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from firecrawl) (15.0.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from firecrawl) (1.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from firecrawl) (3.11.15)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->firecrawl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->firecrawl) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->firecrawl) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->firecrawl) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->firecrawl) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->firecrawl) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->firecrawl) (3.4.2)\n",
            "Downloading selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading firecrawl-2.5.4-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, python-dotenv, outcome, trio, trio-websocket, firecrawl, selenium\n",
            "Successfully installed firecrawl-2.5.4 outcome-1.3.0.post0 python-dotenv-1.1.0 selenium-4.32.0 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromedriver-binary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HterbMxTvEMq",
        "outputId": "58431efd-6c42-4023-ae9c-3993ed2a067b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromedriver-binary\n",
            "  Downloading chromedriver_binary-138.0.7174.0.0.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: chromedriver-binary\n",
            "  Building wheel for chromedriver-binary (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chromedriver-binary: filename=chromedriver_binary-138.0.7174.0.0-py3-none-any.whl size=9010469 sha256=88c5d843526258c2afd2462216197df7e940f571a516050bc52927dbef55b001\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/c3/e4/47b5e3f5f73f05da89c6d4218d05ca161a32026edc8c75c228\n",
            "Successfully built chromedriver-binary\n",
            "Installing collected packages: chromedriver-binary\n",
            "Successfully installed chromedriver-binary-138.0.7174.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import firecrawl\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v_XIfl28rtzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython #Import get_ipython\n"
      ],
      "metadata": {
        "id": "V3pD7mn2vKUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from firecrawl import FirecrawlApp\n",
        "except ImportError:\n",
        "    import pip\n",
        "    pip.main(['install', 'firecrawl'])\n",
        "    from firecrawl import FirecrawlApp\n",
        "\n",
        "# Configure Chrome options for headless browsing\n",
        "def setup_chrome_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    # For Google Colab -  Check if running on Google Colab\n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        # Install chromium, its driver, and selenium\n",
        "        !apt-get update\n",
        "        !apt install chromium-chromedriver\n",
        "        !pip install selenium\n",
        "        # Set the executable path and options\n",
        "        chrome_options.binary_location = \"/usr/bin/chromium-browser\"\n",
        "        # Update webdriver.Chrome with executable_path\n",
        "        return webdriver.Chrome(options=chrome_options) # Updated Line ## repeated error\n",
        "    else:\n",
        "        return webdriver.Chrome(options=chrome_options)"
      ],
      "metadata": {
        "id": "0vdbQsdgt4th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8x_ra8ULJaYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class_crawler\n"
      ],
      "metadata": {
        "id": "wASzBN_kJbXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Defining a class for event crawler\n",
        "\n",
        "class EventCrawler:\n",
        "    \"\"\"\n",
        "    A class for crawling event directories from different trade shows and associations. Supports ISA Sign Expo, Printing United Expo, and various trade associations. \"\"\"\n",
        "\n",
        "    def __init__(self, firecrawl_api_key=None):\n",
        "        \"\"\"Initialize the crawler with optional Firecrawl API key\"\"\"\n",
        "        self.driver = setup_chrome_driver()\n",
        "        self.firecrawl_api_key = firecrawl_api_key\n",
        "        self.firecrawl = None\n",
        "\n",
        "        if firecrawl_api_key:\n",
        "            self.firecrawl = FirecrawlApp(api_key=firecrawl_api_key)\n",
        "\n",
        "        #  source URLs for different events and associations\n",
        "        self.sources = {\n",
        "            \"ISA\": {\n",
        "                \"name\": \"International Sign Association Expo 2025\",\n",
        "                \"url\": \"https://isasignexpo2025.mapyourshow.com/8_0/explore/exhibitor-gallery.cfm\",\n",
        "                \"target_audience\": \"sign shop owners, large-format printers\",\n",
        "                \"link_pattern\": \"exhibitor-details.cfm\"\n",
        "            },\n",
        "\n",
        "            \"SGIA\": {\n",
        "                \"name\": \"Printing United Alliance\",\n",
        "                \"url\": \"https://directory.printing.org/search-results/?org_type_filter=corporate-print-and-promo-distributor&country=US&state=&city=&zip=&search_radius=&lat=&lng=&capabilities=&org_equipment=&freeform_search=\",\n",
        "                \"target_audience\": \"pritners and promoter companies\",\n",
        "                \"link_pattern\": \"member/\"\n",
        "             },\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "    def select_event(self, event_key):\n",
        "        \"\"\"\n",
        "        Select which event/association to crawl\n",
        "\n",
        "        Args:\n",
        "            event_key: Key for the event in self.sources dictionary\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with event information or None if not found\n",
        "        \"\"\"\n",
        "        if event_key in self.sources:\n",
        "            print(f\"Selected: {self.sources[event_key]['name']}\")\n",
        "            print(f\"Target audience: {self.sources[event_key]['target_audience']}\")\n",
        "            return self.sources[event_key]\n",
        "        else:\n",
        "            print(f\"Error: {event_key} not found in available sources\")\n",
        "            print(f\"Available options: {list(self.sources.keys())}\")\n",
        "            return None\n",
        "\n",
        "    def get_company_links(self, event_info, max_pages=1):\n",
        "        \"\"\"\n",
        "        Gather company links from the event directory page\n",
        "\n",
        "        Args:\n",
        "            event_info: Dictionary with event information\n",
        "            max_pages: Maximum number of pagination pages to crawl\n",
        "\n",
        "        Returns:\n",
        "            List of tuples (company_name, company_url)\n",
        "        \"\"\"\n",
        "        base_url = event_info[\"url\"]\n",
        "        link_pattern = event_info[\"link_pattern\"]\n",
        "        company_links = []\n",
        "\n",
        "        print(f\"Crawling directory: {base_url}\")\n",
        "\n",
        "        #  pagination by appending page parameters\n",
        "        for page in range(1, max_pages + 1):\n",
        "            try:\n",
        "                #  events might use different pagination structures\n",
        "                page_url = base_url\n",
        "                if '?' in base_url:\n",
        "                    page_url = f\"{base_url}&page={page}\"\n",
        "                else:\n",
        "                    page_url = f\"{base_url}?page={page}\"\n",
        "\n",
        "                print(f\"Crawling page {page}: {page_url}\")\n",
        "                self.driver.get(page_url)\n",
        "                time.sleep(3)  # Wait for JavaScript to render\n",
        "\n",
        "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "\n",
        "                # Find all links on the page\n",
        "                links = soup.find_all('a', href=True)\n",
        "\n",
        "                page_links = []\n",
        "                for link in links:\n",
        "                    href = link.get('href')\n",
        "                    name = link.text.strip()\n",
        "\n",
        "                    # Only include links that match our pattern and have a name\n",
        "                    if link_pattern in href and name:\n",
        "                        # Ensure the URL is absolute\n",
        "                        if not href.startswith('http'):\n",
        "                            # Extract the domain from the base URL\n",
        "                            from urllib.parse import urlparse\n",
        "                            parsed_url = urlparse(base_url)\n",
        "                            domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                            href = domain + href if href.startswith('/') else domain + '/' + href\n",
        "\n",
        "                        page_links.append((name, href))\n",
        "\n",
        "                # Add unique links to our collection\n",
        "                for link in page_links:\n",
        "                    if link not in company_links:\n",
        "                        company_links.append(link)\n",
        "\n",
        "                print(f\"Found {len(page_links)} companies on page {page}\")\n",
        "\n",
        "\n",
        "                if not page_links:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error crawling page {page}: {e}\")\n",
        "                break\n",
        "\n",
        "        print(f\"Total unique companies found: {len(company_links)}\")\n",
        "        return company_links\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def save_results(self, data, filename_prefix):\n",
        "        \"\"\"\n",
        "        Save results to both CSV and JSON files\n",
        "\n",
        "        Args:\n",
        "            data: List of dictionaries with company data\n",
        "            filename_prefix: Prefix for the output filenames\n",
        "\n",
        "        Returns:\n",
        "            Tuple of filenames (csv_path, json_path)\n",
        "        \"\"\"\n",
        "        if not data:\n",
        "            print(\"No data to save.\")\n",
        "            return None, None\n",
        "\n",
        "        #CSV\n",
        "        csv_path = f\"{filename_prefix}.csv\"\n",
        "        pd.DataFrame(data).to_csv(csv_path, index=False)\n",
        "        print(f\"Data saved to {csv_path}\")\n",
        "\n",
        "        #JSON\n",
        "        json_path = f\"{filename_prefix}.json\"\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"Data saved to {json_path}\")\n",
        "\n",
        "        return csv_path, json_path\n",
        "\n",
        "\n",
        "  ## selenium based web crawler\n",
        "\n",
        "    def scrape_company_details_selenium(self, company_links):\n",
        "        \"\"\"\n",
        "        Scrape company details using Selenium and BeautifulSoup (slower but doesn't require API keys)\n",
        "\n",
        "        Args:\n",
        "            company_links: List of tuples (company_name, company_url)\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries with company data\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i, (name, url) in enumerate(company_links):\n",
        "            try:\n",
        "                print(f\"Scraping {i+1}/{len(company_links)}: {name}\")\n",
        "                self.driver.get(url)\n",
        "                time.sleep(3)  # Let page load\n",
        "\n",
        "                #dismiss overlay if it exists ## after unsuccesful attempts of accessing the \"about company section on company sub-link\n",
        "                try:\n",
        "                    overlay = WebDriverWait(self.driver, 2).until(\n",
        "                        EC.presence_of_element_located((By.CLASS_NAME, \"introjs-overlay\"))\n",
        "                    )\n",
        "                    self.driver.execute_script(\"arguments[0].style.display = 'none';\", overlay)\n",
        "                    print(\" Removed overlay\")\n",
        "                except:\n",
        "                    pass  # No overlay found\n",
        "\n",
        "                #  try to scroll and JS-click \"About\" button\n",
        "                try:\n",
        "                    about_button = WebDriverWait(self.driver, 5).until(\n",
        "                        EC.presence_of_element_located((By.XPATH, \"//button[contains(text(), 'About')]\"))\n",
        "                    )\n",
        "                    self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", about_button)\n",
        "                    self.driver.execute_script(\"arguments[0].click();\", about_button)\n",
        "                    time.sleep(1)\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not click 'About' button for {name}: {e}\")\n",
        "\n",
        "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "\n",
        "                company_data = {\n",
        "                    \"Company Name\": name,\n",
        "                    \"Profile URL\": url,\n",
        "                    \"Description\": \"\",\n",
        "                    \"Website\": \"\",\n",
        "                    \"Phone\": \"\",\n",
        "                    \"Email\": \"\",\n",
        "                    \"Address\": \"\",\n",
        "                    \"Categories\": \"\",\n",
        "                    \"Social Media\": \"\"\n",
        "                }\n",
        "\n",
        "                #  Extract expanded description (after click)\n",
        "                desc_block = soup.select_one('#section-description .line-clamp_10')\n",
        "                if desc_block:\n",
        "                    company_data[\"Description\"] = desc_block.get_text(strip=True)\n",
        "                else:\n",
        "                    # Fallback: Try known static selectors\n",
        "                    for selector in ['.company-description', '.exhibitor-description', '.member-description', '.description', '#description']:\n",
        "                        desc = soup.select_one(selector)\n",
        "                        if desc:\n",
        "                            company_data[\"Description\"] = desc.get_text(strip=True)\n",
        "                            break\n",
        "\n",
        "                # Website\n",
        "                for selector in ['a.website', 'a.company-website', '.website a', '.contact a[href^=\"http\"]']:\n",
        "                    website = soup.select_one(selector)\n",
        "                    if website and website.has_attr('href'):\n",
        "                        company_data[\"Website\"] = website['href']\n",
        "                        break\n",
        "\n",
        "                # Phone\n",
        "                for selector in ['.phone', '.contact .phone', '.company-phone', '.tel']:\n",
        "                    phone = soup.select_one(selector)\n",
        "                    if phone:\n",
        "                        company_data[\"Phone\"] = phone.get_text(strip=True).replace(\"Phone:\", \"\").strip()\n",
        "                        break\n",
        "\n",
        "                # Email\n",
        "                for selector in ['.email', '.contact .email', '.company-email', 'a[href^=\"mailto:\"]']:\n",
        "                    email = soup.select_one(selector)\n",
        "                    if email:\n",
        "                        if email.has_attr('href') and 'mailto:' in email['href']:\n",
        "                            company_data[\"Email\"] = email['href'].replace(\"mailto:\", \"\").strip()\n",
        "                        else:\n",
        "                            company_data[\"Email\"] = email.get_text(strip=True).replace(\"Email:\", \"\").strip()\n",
        "                        break\n",
        "\n",
        "                # Address\n",
        "                for selector in ['.address', '.company-address', '.location', '.contact-info address']:\n",
        "                    address = soup.select_one(selector)\n",
        "                    if address:\n",
        "                        company_data[\"Address\"] = address.get_text(strip=True).replace(\"Address:\", \"\").strip()\n",
        "                        break\n",
        "\n",
        "                # Categories\n",
        "                for selector in ['.categories', '.exhibitor-categories', '.company-categories', '.tags']:\n",
        "                    categories = soup.select_one(selector)\n",
        "                    if categories:\n",
        "                        cats = [c.get_text(strip=True) for c in categories.select('.category, .tag, .cat')]\n",
        "                        if cats:\n",
        "                            company_data[\"Categories\"] = ', '.join(cats)\n",
        "                        else:\n",
        "                            company_data[\"Categories\"] = categories.get_text(strip=True)\n",
        "                        break\n",
        "\n",
        "                # Social Media\n",
        "                social_links = []\n",
        "                for platform in ['facebook', 'twitter', 'linkedin', 'instagram']:\n",
        "                    for link in soup.find_all('a', href=True):\n",
        "                        if platform in link.get('href', '').lower():\n",
        "                            social_links.append(link.get('href'))\n",
        "                if social_links:\n",
        "                    company_data[\"Social Media\"] = ', '.join(social_links)\n",
        "\n",
        "                results.append(company_data)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" Error scraping {name}: {e}\")\n",
        "\n",
        "        return results #Fixed the indentation of this line\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # def cleanup(self):\n",
        "    #     \"\"\"Close the browser when done\"\"\"\n",
        "    #     if self.driver:\n",
        "    #         self.driver.quit()\n",
        "\n",
        "  # def analyze_results(self, data):\n",
        "    #     \"\"\"\n",
        "    #     Perform basic analysis on the scraped data\n",
        "\n",
        "    #     Args:\n",
        "    #         data: List of dictionaries with company data\n",
        "\n",
        "    #     Returns:\n",
        "    #         Dictionary with analysis results\n",
        "    #     \"\"\"\n",
        "    #     df = pd.DataFrame(data)\n",
        "\n",
        "    #     analysis = {\n",
        "    #         \"total_companies\": len(df),\n",
        "    #         \"companies_with_website\": df[\"Website\"].notna().sum(),\n",
        "    #         \"companies_with_email\": df[\"Email\"].notna().sum() if \"Email\" in df.columns else 0,\n",
        "    #         \"companies_with_phone\": df[\"Phone\"].notna().sum(),\n",
        "    #         \"most_common_categories\": None\n",
        "    #     }\n",
        "\n",
        "    #     # If we have categories, analyze them\n",
        "    #     if \"Categories\" in df.columns:\n",
        "    #         # Split categories and count frequencies\n",
        "    #         all_categories = []\n",
        "    #         for cats in df[\"Categories\"].dropna():\n",
        "    #             categories = [c.strip() for c in cats.split(',')]\n",
        "    #             all_categories.extend(categories)\n",
        "\n",
        "    #         if all_categories:\n",
        "    #             from collections import Counter\n",
        "    #             category_counts = Counter(all_categories)\n",
        "    #             analysis[\"most_common_categories\"] = category_counts.most_common(10)\n",
        "\n",
        "    #     return analysis"
      ],
      "metadata": {
        "id": "yJhfBtbxt69y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### testing updated selenium_crawler for about company section\n",
        "\n",
        "# Sample test snippet (outside class)\n",
        " # or replace with local file import\n",
        "\n",
        "test_links = [\n",
        "    (\"Murdoch Engineering\", \"https://isasignexpo2025.mapyourshow.com/8_0/exhibitor/exhibitor-details.cfm?exhid=1381\"),\n",
        "    (\"Epilog Laser\", \"https://isasignexpo2025.mapyourshow.com/8_0/exhibitor/exhibitor-details.cfm?exhid=1030\"),\n",
        "    (\"Zund America\", \"https://isasignexpo2025.mapyourshow.com/8_0/exhibitor/exhibitor-details.cfm?exhid=967\")\n",
        "]\n",
        "\n",
        "crawler = EventCrawler()\n",
        "data = crawler.scrape_company_details_selenium(test_links)\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-oyhac5j-qBw",
        "outputId": "f05a04ea-6b4e-48c4-edde-04184e95fa97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.32.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
            "Scraping 1/3: Murdoch Engineering\n",
            " Removed overlay\n",
            "Scraping 2/3: Epilog Laser\n",
            " Removed overlay\n",
            "Could not click 'About' button for Epilog Laser: Message: \n",
            "Stacktrace:\n",
            "#0 0x57723953978a <unknown>\n",
            "#1 0x577238fdc0a0 <unknown>\n",
            "#2 0x57723902d9b0 <unknown>\n",
            "#3 0x57723902dba1 <unknown>\n",
            "#4 0x57723907bea4 <unknown>\n",
            "#5 0x5772390533cd <unknown>\n",
            "#6 0x5772390792a0 <unknown>\n",
            "#7 0x577239053173 <unknown>\n",
            "#8 0x57723901fd4b <unknown>\n",
            "#9 0x5772390209b1 <unknown>\n",
            "#10 0x5772394fe93b <unknown>\n",
            "#11 0x57723950283a <unknown>\n",
            "#12 0x5772394e6692 <unknown>\n",
            "#13 0x5772395033c4 <unknown>\n",
            "#14 0x5772394cb4cf <unknown>\n",
            "#15 0x577239527568 <unknown>\n",
            "#16 0x577239527746 <unknown>\n",
            "#17 0x5772395385f6 <unknown>\n",
            "#18 0x7e498e978ac3 <unknown>\n",
            "\n",
            "Scraping 3/3: Zund America\n",
            " Removed overlay\n",
            "Could not click 'About' button for Zund America: Message: \n",
            "Stacktrace:\n",
            "#0 0x57723953978a <unknown>\n",
            "#1 0x577238fdc0a0 <unknown>\n",
            "#2 0x57723902d9b0 <unknown>\n",
            "#3 0x57723902dba1 <unknown>\n",
            "#4 0x57723907bea4 <unknown>\n",
            "#5 0x5772390533cd <unknown>\n",
            "#6 0x5772390792a0 <unknown>\n",
            "#7 0x577239053173 <unknown>\n",
            "#8 0x57723901fd4b <unknown>\n",
            "#9 0x5772390209b1 <unknown>\n",
            "#10 0x5772394fe93b <unknown>\n",
            "#11 0x57723950283a <unknown>\n",
            "#12 0x5772394e6692 <unknown>\n",
            "#13 0x5772395033c4 <unknown>\n",
            "#14 0x5772394cb4cf <unknown>\n",
            "#15 0x577239527568 <unknown>\n",
            "#16 0x577239527746 <unknown>\n",
            "#17 0x5772395385f6 <unknown>\n",
            "#18 0x7e498e978ac3 <unknown>\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Company Name                                        Profile URL  \\\n",
              "0  Murdoch Engineering  https://isasignexpo2025.mapyourshow.com/8_0/ex...   \n",
              "1         Epilog Laser  https://isasignexpo2025.mapyourshow.com/8_0/ex...   \n",
              "2         Zund America  https://isasignexpo2025.mapyourshow.com/8_0/ex...   \n",
              "\n",
              "  Description                                            Website Phone Email  \\\n",
              "0                                                                              \n",
              "1                                                                              \n",
              "2              https://www.linkedin.com/company/zund-america-...               \n",
              "\n",
              "                                             Address Categories  \\\n",
              "0  2399 Hwy 34Bldg. A-2ManasquanNJ 08736United St...              \n",
              "1  16371 Table Mountain PkwyGoldenCO  80403-1826U...              \n",
              "2  8142 South 6th StreetOak CreekWI  53154United ...              \n",
              "\n",
              "                                        Social Media  \n",
              "0                                                     \n",
              "1                                                     \n",
              "2  https://www.facebook.com/ZundAmericaInc/, http...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ad0338c-3119-415f-9377-7681d2099405\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Company Name</th>\n",
              "      <th>Profile URL</th>\n",
              "      <th>Description</th>\n",
              "      <th>Website</th>\n",
              "      <th>Phone</th>\n",
              "      <th>Email</th>\n",
              "      <th>Address</th>\n",
              "      <th>Categories</th>\n",
              "      <th>Social Media</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Murdoch Engineering</td>\n",
              "      <td>https://isasignexpo2025.mapyourshow.com/8_0/ex...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>2399 Hwy 34Bldg. A-2ManasquanNJ 08736United St...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Epilog Laser</td>\n",
              "      <td>https://isasignexpo2025.mapyourshow.com/8_0/ex...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>16371 Table Mountain PkwyGoldenCO  80403-1826U...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Zund America</td>\n",
              "      <td>https://isasignexpo2025.mapyourshow.com/8_0/ex...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.linkedin.com/company/zund-america-...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>8142 South 6th StreetOak CreekWI  53154United ...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.facebook.com/ZundAmericaInc/, http...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ad0338c-3119-415f-9377-7681d2099405')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0ad0338c-3119-415f-9377-7681d2099405 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0ad0338c-3119-415f-9377-7681d2099405');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2279cfef-9df3-4e11-90be-52761544a0e0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2279cfef-9df3-4e11-90be-52761544a0e0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2279cfef-9df3-4e11-90be-52761544a0e0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Company Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Murdoch Engineering\",\n          \"Epilog Laser\",\n          \"Zund America\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Profile URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"https://isasignexpo2025.mapyourshow.com/8_0/exhibitor/exhibitor-details.cfm?exhid=1381\",\n          \"https://isasignexpo2025.mapyourshow.com/8_0/exhibitor/exhibitor-details.cfm?exhid=1030\",\n          \"https://isasignexpo2025.mapyourshow.com/8_0/exhibitor/exhibitor-details.cfm?exhid=967\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Website\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"https://www.linkedin.com/company/zund-america-inc/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Phone\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Address\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2399 Hwy 34Bldg. A-2ManasquanNJ 08736United States of America\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Social Media\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"https://www.facebook.com/ZundAmericaInc/, https://www.facebook.com/ZundAmericaInc/, https://www.linkedin.com/company/zund-america-inc/, https://www.linkedin.com/company/zund-america-inc/, https://www.instagram.com/zundcutter_ch/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Optionally: store API key from .env or just set as None\n",
        "    FIRECRAWL_API_KEY = None  # set to your key if using Firecrawl later\n",
        "\n",
        "    crawler = EventCrawler(firecrawl_api_key=FIRECRAWL_API_KEY)\n",
        "\n",
        "    print(\"Available events/associations to crawl:\")\n",
        "    for key in crawler.sources:\n",
        "        print(f\"- {key}: {crawler.sources[key]['name']}\")\n",
        "\n",
        "    event_key = input(\"Enter the event key to crawl (e.g., ISA): \").strip().upper()\n",
        "    event_info = crawler.select_event(event_key)\n",
        "    if not event_info:\n",
        "        return\n",
        "\n",
        "    # Step 1: Get links to all company detail pages\n",
        "    company_links = crawler.get_company_links(event_info, max_pages=1)  # Change max_pages if needed\n",
        "    if not company_links:\n",
        "        print(\"No company links found. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Total companies found: {len(company_links)}\")\n",
        "\n",
        "    # Step 2: Use Selenium to scrape details from those links\n",
        "    print(\"Using Selenium to extract company details...\")\n",
        "    company_data = crawler.scrape_company_details_selenium(company_links)\n",
        "\n",
        "    # Step 3: Save the result\n",
        "    output_prefix = f\"{event_key}_companies_{time.strftime('%Y%m%d')}\"\n",
        "    crawler.save_results(company_data, output_prefix)\n",
        "\n",
        "    # Skipping analysis and cleanup for this version\n",
        "    # analysis = crawler.analyze_results(company_data)\n",
        "    # for key, value in analysis.items():\n",
        "    #     print(f\"{key}: {value}\")\n",
        "\n",
        "    # crawler.cleanup()  # Skip closing the driver for now\n",
        "\n",
        "# Standard entry point\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxoZNOnV33Os",
        "outputId": "863f0aee-aa13-4025-eb39-323d925b59bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.32.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
            "Available events/associations to crawl:\n",
            "- ISA: International Sign Association Expo 2025\n",
            "- SGIA: Printing United Alliance\n",
            "Enter the event key to crawl (e.g., ISA): ISA\n",
            "Selected: International Sign Association Expo 2025\n",
            "Target audience: sign shop owners, large-format printers\n",
            "Crawling directory: https://isasignexpo2025.mapyourshow.com/8_0/explore/exhibitor-gallery.cfm\n",
            "Crawling page 1: https://isasignexpo2025.mapyourshow.com/8_0/explore/exhibitor-gallery.cfm?page=1\n",
            "Found 75 companies on page 1\n",
            "Total unique companies found: 73\n",
            "Total companies found: 73\n",
            "Using Selenium to extract company details...\n",
            "Scraping 1/73: Almaas Aldahabi Plastic Industries for P.V.C Foam board production L.L.C\n",
            "Scraping 2/73: AP Lazer | Laser Cutters + Engravers\n",
            "Scraping 3/73: CUTWORX USA\n",
            "Scraping 4/73: Elliott Equipment Company\n",
            "Scraping 5/73: Epilog Laser\n",
            "Scraping 6/73: General Formulations\n",
            "Scraping 7/73: Gildo Profilati\n",
            "Scraping 8/73: Hirsch Solutions, LLC\n",
            "Scraping 9/73: InfoFlo Print\n",
            "Scraping 10/73: Laguna Tools Inc.\n",
            "Scraping 11/73: Lintec of America, Inc.\n",
            "Scraping 12/73: Magnum Magnetics Corporation\n",
            "Scraping 13/73: Murdoch Engineering\n",
            "Scraping 14/73: Permits by Mirza Corp. dba Permit Ninjas\n",
            "Scraping 15/73: Photo Tex Group Inc.\n",
            "Scraping 16/73: Principal Industries\n",
            "Scraping 17/73: SDS Automation\n",
            "Scraping 18/73: Signage Details\n",
            "Scraping 19/73: StratoJet\n",
            "Scraping 20/73: Stud Welding Products Inc.\n",
            "Scraping 21/73: Trotec Laser\n",
            "Scraping 22/73: Vibrant Print Solutions\n",
            "Scraping 23/73: Vytek (Vinyl Technologies)\n",
            "Scraping 24/73: X-Edge Products, Inc\n",
            "Scraping 25/73: Zund America\n",
            "Scraping 26/73: 1 Source BPO\n",
            "Scraping 27/73: 3A Composites USA, Inc.\n",
            "Scraping 28/73: 3M Commercial Solutions\n",
            "Scraping 29/73: 4Ever Products\n",
            "Scraping 30/73: 4over LLC\n",
            "Scraping 31/73: 4U Media & Displays, LLC\n",
            "Scraping 32/73: 7C Digital Printing Textile Co.,Ltd\n",
            "Scraping 33/73: A&C Plastics\n",
            "Scraping 34/73: A.R.K. Ramos Foundry & Mfg. Co.\n",
            "Scraping 35/73: AA ACRYLIC CO.，LTD\n",
            "Scraping 36/73: AA LED Supply Corp.\n",
            "Scraping 37/73: ABC Sign Products\n",
            "Scraping 38/73: Abitech\n",
            "Scraping 39/73: ABLE\n",
            "Scraping 40/73: Action Lighting\n",
            "Scraping 41/73: ADHES PACKAGING TECHNOLOGY (MALAYSIA) SDN.BHD.\n",
            "Scraping 42/73: ADMAX Exhibit & Display Ltd.\n",
            "Scraping 43/73: Advanced Film Technology\n",
            "Scraping 44/73: Advanced Greig Laminators, Inc.\n",
            "Scraping 45/73: Advanced Modern Sign\n",
            "Scraping 46/73: Advantage Innovations, Inc\n",
            "Scraping 47/73: Aeon Laser\n",
            "Scraping 48/73: Agfa Corporation\n",
            "Scraping 49/73: AgiLight\n",
            "Scraping 50/73: AkzoNobel\n",
            "Scraping 51/73: Alfa(Wuxi) Industrial Co.,Ltd\n",
            "Scraping 52/73: Allanson International\n",
            "Scraping 53/73: Alliance Technology Corp.  & SCG DIGITAL AMERICA INC.\n",
            "Scraping 54/73: ALLSIGN INTERNATIONAL TECHNOLOGY CO.,LTD\n",
            "Scraping 55/73: Alro Steel & Plastics\n",
            "Scraping 56/73: Altec Industries\n",
            "Scraping 57/73: Aludecor\n",
            "Scraping 58/73: Alumanate ACM\n",
            "Scraping 59/73: American Ground Screw, Inc.\n",
            "Scraping 60/73: Anhui Angran Green Technology Co.,Ltd.\n",
            "Scraping 61/73: Anhui sunsign AD co., ltd\n",
            "Scraping 62/73: AnyLux Co, Ltd\n",
            "Scraping 63/73: AOMYA DIGITAL TECHNOLOGY CO., LTD.\n",
            "Scraping 64/73: Arcus Printers\n",
            "Scraping 65/73: Arizona Sign Association\n",
            "Scraping 66/73: Arlon Graphics\n",
            "Scraping 67/73: Art Sign Works\n",
            "Scraping 68/73: Arter Neon Sign Inc\n",
            "Scraping 69/73: Ascent Equipment\n",
            "Scraping 70/73: Ascentium Capital\n",
            "Scraping 71/73: Aurora Science and Technology Co., Ltd\n",
            "Scraping 72/73: Avery Dennison Graphics Solutions\n",
            "Scraping 73/73: Awning and Sign Contractors\n",
            "Data saved to ISA_companies_20250513.csv\n",
            "Data saved to ISA_companies_20250513.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_company_details_firecrawl(self, company_links, method='batch'):\n",
        "        \"\"\"\n",
        "        Scrape company details using Firecrawl API\n",
        "\n",
        "        Args:\n",
        "            company_links: List of tuples (company_name, company_url)\n",
        "            method: 'batch' for batch processing or 'individual' for one-by-one\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries with company data\n",
        "        \"\"\"\n",
        "        if not self.firecrawl:\n",
        "            print(\"Firecrawl API key not provided. Please initialize with a valid API key.\")\n",
        "            return []\n",
        "\n",
        "        results = []\n",
        "\n",
        "        if method == 'batch':\n",
        "            # Extract just the URLs for batch processing\n",
        "            urls = [url for _, url in company_links]\n",
        "\n",
        "            print(f\"Batch processing {len(urls)} company pages...\")\n",
        "\n",
        "            # The NLP prompt to extract structured data\n",
        "            prompt = \"\"\"\n",
        "            Extract the following information about this company:\n",
        "            1. Full company name\n",
        "            2. Description/About text\n",
        "            3. Website URL\n",
        "            4. Phone number\n",
        "            5. Email address\n",
        "            6. Physical address\n",
        "            7. Product/service categories\n",
        "            8. Social media links\n",
        "            9. Company size (if available)\n",
        "            10. Year founded (if available)\n",
        "            11. Any additional contact information\n",
        "            \"\"\"\n",
        "\n",
        "            # Use Firecrawl's batch processing\n",
        "            batch_results = self.firecrawl.batch_scrape_urls(\n",
        "                urls,\n",
        "                formats=['json'],\n",
        "                json_options={'prompt': prompt}\n",
        "            )\n",
        "\n",
        "            # Process the results\n",
        "            for i, (name, url) in enumerate(company_links):\n",
        "                try:\n",
        "                    if url in batch_results:\n",
        "                        result = batch_results[url]\n",
        "\n",
        "                        # Create base company data\n",
        "                        company_data = {\n",
        "                            \"Company Name\": name,\n",
        "                            \"Profile URL\": url\n",
        "                        }\n",
        "\n",
        "                        # Add the extracted JSON data\n",
        "                        if 'json' in result:\n",
        "                            company_data.update(result['json'])\n",
        "\n",
        "                        results.append(company_data)\n",
        "                        print(f\"Processed {i+1}/{len(company_links)}: {name}\")\n",
        "                    else:\n",
        "                        print(f\"Failed to scrape {name} at {url}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {name}: {e}\")\n",
        "\n",
        "        else:  # Individual processing\n",
        "            for i, (name, url) in enumerate(company_links):\n",
        "                try:\n",
        "                    print(f\"Scraping {i+1}/{len(company_links)}: {name}\")\n",
        "\n",
        "                    # The NLP prompt to extract structured data\n",
        "                    prompt = \"\"\"\n",
        "                    Extract the following information about this company:\n",
        "                    1. Full company name\n",
        "                    2. Description/About text\n",
        "                    3. Website URL\n",
        "                    4. Phone number\n",
        "                    5. Email address\n",
        "                    6. Physical address\n",
        "                    7. Product/service categories\n",
        "                    8. Social media links\n",
        "                    9. Company size (if available)\n",
        "                    10. Year founded (if available)\n",
        "                    11. Any additional contact information\n",
        "                    \"\"\"\n",
        "\n",
        "                    # Use Firecrawl to extract data\n",
        "                    result = self.firecrawl.scrape_url(\n",
        "                        url,\n",
        "                        formats=['json'],\n",
        "                        json_options={'prompt': prompt}\n",
        "                    )\n",
        "\n",
        "                    # Create base company data\n",
        "                    company_data = {\n",
        "                        \"Company Name\": name,\n",
        "                        \"Profile URL\": url\n",
        "                    }\n",
        "\n",
        "                    # Add the extracted JSON data\n",
        "                    if hasattr(result, 'json') and result.json:\n",
        "                        company_data.update(result.json)\n",
        "\n",
        "                    results.append(company_data)\n",
        "                    print(f\"Successfully scraped: {name}\")\n",
        "\n",
        "                    # Add a small delay to avoid rate limiting\n",
        "                    time.sleep(1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error scraping {name}: {e}\")\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "9cDE0-MK3SSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enrichment Agent\n",
        "\n"
      ],
      "metadata": {
        "id": "gYwtchPEK66z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## relevant imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import logging\n",
        "from dotenv import load_dotenv\n",
        "from typing import Dict, List, Optional, Any\n"
      ],
      "metadata": {
        "id": "7xtnA0lLLpUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load environment variables\n",
        "# load_dotenv()\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SalesLeadEnrichmentAgent:\n",
        "    \"\"\"\n",
        "    Agent for enriching sales lead data with additional company and contact information.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # API Keys (store these in .env file)\n",
        "        self.firecrawl_api_key = os.getenv(\"<abc>\", \"\")\n",
        "        # self.clearbit_api_key = os.getenv(\"CLEARBIT_API_KEY\", \"\")\n",
        "        self.hunter_api_key = os.getenv(\"HUNTER_API_KEY\", \"\")\n",
        "        self.serp_api_key = os.getenv(\"\", \"\")\n",
        "\n",
        "        # Validate required API keys\n",
        "        if not self.firecrawl_api_key:\n",
        "            logger.warning(\"Firecrawl API key not found. Some features may be limited.\")\n",
        "\n",
        "        # Headers for requests\n",
        "        self.headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "    def parse_input_data(self, raw_data: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Parse raw input data into structured format.\n",
        "        \"\"\"\n",
        "        company_data = {}\n",
        "\n",
        "        # Extract company name\n",
        "        company_name_match = re.search(r'\\*\\*Company Name\\*\\*\\s*([^\\n]+)', raw_data)\n",
        "        if company_name_match:\n",
        "            company_data[\"company_name\"] = company_name_match.group(1).strip()\n",
        "\n",
        "        # Extract profile URL\n",
        "        profile_url_match = re.search(r'\\*\\*Profile URL\\*\\*\\s*(https?://[^\\s]+)', raw_data)\n",
        "        if profile_url_match:\n",
        "            company_data[\"profile_url\"] = profile_url_match.group(1).strip()\n",
        "\n",
        "        # Extract description\n",
        "        description_match = re.search(r'\\*\\*Description\\*\\*\\s*([^\\n]*)', raw_data)\n",
        "        if description_match:\n",
        "            company_data[\"description\"] = description_match.group(1).strip()\n",
        "\n",
        "        # Extract website\n",
        "        website_match = re.search(r'\\*\\*Website\\*\\*\\s*(https?://[^\\s\\n]*)', raw_data)\n",
        "        if website_match:\n",
        "            company_data[\"website\"] = website_match.group(1).strip()\n",
        "\n",
        "        # Extract phone\n",
        "        phone_match = re.search(r'\\*\\*Phone\\*\\*\\s*([^\\n]*)', raw_data)\n",
        "        if phone_match:\n",
        "            company_data[\"phone\"] = phone_match.group(1).strip()\n",
        "\n",
        "        # Extract email\n",
        "        email_match = re.search(r'\\*\\*Email\\*\\*\\s*([^\\n]*)', raw_data)\n",
        "        if email_match:\n",
        "            company_data[\"email\"] = email_match.group(1).strip()\n",
        "\n",
        "        # Extract address\n",
        "        address_pattern = r'\\*\\*Address\\*\\*\\s*([^\\n]*(?:\\n[^\\n*]+)*)'\n",
        "        address_match = re.search(address_pattern, raw_data)\n",
        "        if address_match:\n",
        "            company_data[\"address\"] = address_match.group(1).strip()\n",
        "\n",
        "        # Extract categories\n",
        "        categories_match = re.search(r'\\*\\*Categories\\*\\*\\s*([^\\n]*)', raw_data)\n",
        "        if categories_match:\n",
        "            company_data[\"categories\"] = categories_match.group(1).strip()\n",
        "\n",
        "        # Extract social media\n",
        "        social_media_pattern = r'\\*\\*Social Media\\*\\*\\s*((?:https?://[^\\s,]+(?:,\\s*)?)+)'\n",
        "        social_media_match = re.search(social_media_pattern, raw_data)\n",
        "        if social_media_match:\n",
        "            social_media_str = social_media_match.group(1).strip()\n",
        "            company_data[\"social_media\"] = [url.strip() for url in re.findall(r'https?://[^\\s,]+', social_media_str)]\n",
        "        else:\n",
        "            # Try to find social media links from the rest of the text\n",
        "            company_data[\"social_media\"] = [url.strip() for url in re.findall(r'https?://(?:www\\.)?(?:facebook|linkedin|twitter|instagram|youtube)\\.com/[^\\s,]+', raw_data)]\n",
        "\n",
        "        # If no structured data was found, try to extract information from the raw text\n",
        "        if not company_data:\n",
        "            lines = raw_data.strip().split('\\n')\n",
        "            company_data[\"company_name\"] = lines[0].strip() if lines else \"\"\n",
        "\n",
        "            # Find URLs\n",
        "            urls = re.findall(r'https?://[^\\s]+', raw_data)\n",
        "            if urls:\n",
        "                for url in urls:\n",
        "                    if \"linkedin.com\" in url:\n",
        "                        company_data[\"linkedin\"] = url\n",
        "                    elif \"facebook.com\" in url:\n",
        "                        company_data[\"facebook\"] = url\n",
        "                    elif \"instagram.com\" in url:\n",
        "                        company_data[\"instagram\"] = url\n",
        "                    elif \"twitter.com\" in url:\n",
        "                        company_data[\"twitter\"] = url\n",
        "                    elif \"mapyourshow.com\" in url or \"expo\" in url:\n",
        "                        company_data[\"profile_url\"] = url\n",
        "                    else:\n",
        "                        company_data.setdefault(\"website\", url)\n",
        "\n",
        "            # Try to extract address\n",
        "            address_lines = []\n",
        "            for line in lines:\n",
        "                if re.search(r'\\d+|\\bstreet\\b|\\bave\\b|\\broad\\b|\\bblvd\\b|\\bcity\\b|\\bstate\\b|\\bcountry\\b', line.lower()):\n",
        "                    address_lines.append(line)\n",
        "            if address_lines:\n",
        "                company_data[\"address\"] = \" \".join(address_lines)\n",
        "\n",
        "        # Extract domain from website or social media\n",
        "        company_data[\"domain\"] = self._extract_domain_from_data(company_data)\n",
        "\n",
        "        return company_data\n",
        "\n",
        "    def _extract_domain_from_data(self, company_data: Dict[str, Any]) -> str:\n",
        "        \"\"\"Extract domain from website or social media links\"\"\"\n",
        "        if \"website\" in company_data and company_data[\"website\"]:\n",
        "            parsed_url = urlparse(company_data[\"website\"])\n",
        "            return parsed_url.netloc\n",
        "\n",
        "        # Try to find a company website from social media\n",
        "        if \"social_media\" in company_data and company_data[\"social_media\"]:\n",
        "            for url in company_data[\"social_media\"]:\n",
        "                # Skip social media domains\n",
        "                if not any(sm in url for sm in [\"facebook.com\", \"linkedin.com\", \"twitter.com\", \"instagram.com\", \"youtube.com\"]):\n",
        "                    parsed_url = urlparse(url)\n",
        "                    return parsed_url.netloc\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    # def enrich_with_firecrawl(self, company_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    #     \"\"\"\n",
        "    #     
        "\n",
        "    #     try:\n",
        "    #         # Construct a prompt for extracting company information\n",
        "    #         company_name = company_data.get(\"company_name\", \"\")\n",
        "    #         company_description = company_data.get(\"description\", \"\")\n",
        "    #         domain = company_data.get(\"domain\", \"\")\n",
        "    #         website = company_data.get(\"website\", \"\")\n",
        "\n",
        "    #         prompt = f\"\"\"\n",
        "    #         I need information about a company called {company_name}.\n",
        "\n",
        "    #         Here's what I already know:\n",
        "    #         - Website: {website}\n",
        "    #         - Domain: {domain}\n",
        "    #         - Description: {company_description}\n",
        "\n",
        "    #         Please provide the following details about this company:\n",
        "    #         1. Industry\n",
        "    #         2. Products/Services offered\n",
        "    #         3. Estimated annual revenue\n",
        "    #         4. Number of employees\n",
        "    #         5. Year founded\n",
        "    #         6. Key decision makers (e.g., VPs, Directors, etc.)\n",
        "\n",
        "    #         Format your response as a JSON object with these fields. If you can't determine a value with confidence, set it to null.\n",
        "    #         \"\"\"\n",
        "\n",
        "    #         # Query Firecrawl API\n",
        "    #         url = \"https://api.firecrawl.ai/query\"\n",
        "    #         payload = {\n",
        "    #             \"model\": \"firefunction-v1\",\n",
        "    #             \"prompt\": prompt,\n",
        "    #             \"temperature\": 0.1,\n",
        "    #             \"max_tokens\": 1024\n",
        "    #         }\n",
        "    #      
        "\n",
        "    #         response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "    #         if response.status_code == 200:\n",
        "    #             result = response.json()\n",
        "    #             if 'output' in result:\n",
        "    #                 # Extract JSON from the response\n",
        "    #                 try:\n",
        "    #                     # First, try to parse the entire output as JSON\n",
        "    #                     enriched_data = json.loads(result['output'])\n",
        "    #                 except json.JSONDecodeError:\n",
        "    #                     # If that fails, try to extract JSON from the text\n",
        "    #                     json_match = re.search(r'\\{.*\\}', result['output'], re.DOTALL)\n",
        "    #                     if json_match:\n",
        "    #                 
        "    #                 # Update company data with enriched information\n",
        "    #                 company_data[\"industry\"] = enriched_data.get(\"industry\", \"\")\n",
        "    #                 company_data[\"products\"] = enriched_data.get(\"products\", \"\")\n",
        "    #                 company_data[\"revenue\"] = enriched_data.get(\"revenue\", \"\")\n",
        "    #                 company_data[\"employees\"] = enriched_data.get(\"employees\", \"\")\n",
        "    #                 company_data[\"year_founded\"] = enriched_data.get(\"year_founded\", \"\")\n",
        "    #                 company_data[\"decision_makers\"] = enriched_data.get(\"decision_makers\", [])\n",
        "\n",
        "    #   
        "\n",
        "    #     return company_data\n",
        "\n",
        "    # def enrich_with_clearbit(self, company_data: Dict[str, Any]) -> Dict[str, Any]:  ### OR ANY OTHER 3RD PARTY BUSINESS DATABASE PROVIDER such as Clay\n",
        "    #     \"\"\"\n",
        "    #     Enrich company data using Clearbit's free tier -??\n",
        "    #     \"\"\"\n",
        "    #     if not self.clearbit_api_key:\n",
        "    #         logger.warning(\"Clearbit API key not available. Skipping enrichment.\")\n",
        "    #         return company_data\n",
        "\n",
        "    #     try:\n",
        "    #         domain = company_data.get(\"domain\", \"\")\n",
        "    #         if not domain:\n",
        "    #             logger.warning(\"No domain available for Clearbit enrichment\")\n",
        "    #             return company_data\n",
        "\n",
        "    #         # Query Clearbit API\n",
        "    #         url = f\"https://company.clearbit.com/v2/companies/find?domain={domain}\"\n",
        "    #         headers = {\n",
        "    #             \"Authorization\": f\"Bearer {self.clearbit_api_key}\"\n",
        "    #         }\n",
        "\n",
        "    #         response = requests.get(url, headers=headers)\n",
        "\n",
        "    #         if response.status_code == 200:\n",
        "    #             result = response.json()\n",
        "\n",
        "    #             # Update company data with Clearbit information\n",
        "    #             company_data[\"industry\"] = result.get(\"category\", {}).get(\"industry\", company_data.get(\"industry\", \"\"))\n",
        "    #             company_data[\"description\"] = result.get(\"description\", company_data.get(\"description\", \"\"))\n",
        "    #             company_data[\"year_founded\"] = result.get(\"foundedYear\", company_data.get(\"year_founded\", \"\"))\n",
        "    #             company_data[\"employees\"] = result.get(\"metrics\", {}).get(\"employees\", company_data.get(\"employees\", \"\"))\n",
        "    #             company_data[\"revenue\"] = result.get(\"metrics\", {}).get(\"estimatedAnnualRevenue\", company_data.get(\"revenue\", \"\"))\n",
        "    #             company_data[\"linkedin_url\"] = result.get(\"linkedin\", {}).get(\"handle\", \"\")\n",
        "    #             company_data[\"location\"] = result.get(\"location\", \"\")\n",
        "\n",
        "    #             logger.info(\"Successfully enriched data with Clearbit API\")\n",
        "    #         else:\n",
        "    #             logger.warning(f\"Error from Clearbit API: {response.status_code} - {response.text}\")\n",
        "\n",
        "    #     except Exception as e:\n",
        "    #         logger.error(f\"Error enriching with Clearbit: {str(e)}\")\n",
        "\n",
        "    #     return company_data\n",
        "\n",
        "    # def enrich_with_hunter(self, company_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    #     \"\"\"\n",
        "    #     Find email patterns and contacts using Hunter.io's free tier.\n",
        "    #     \"\"\"\n",
        "    #     if not self.hunter_api_key:\n",
        "    #         logger.warning(\"Hunter API key not available. Skipping enrichment.\")\n",
        "    #         return company_data\n",
        "\n",
        "    #     try:\n",
        "    #         domain = company_data.get(\"domain\", \"\")\n",
        "    #         if not domain:\n",
        "    #             logger.warning(\"No domain available for Hunter.io enrichment\")\n",
        "    #             return company_data\n",
        "\n",
        "    #         # Query Hunter.io API for email pattern\n",
        "    #         url = f\"https://api.hunter.io/v2/domain-search?domain={domain}&api_key={self.hunter_api_key}\"\n",
        "\n",
        "    #         response = requests.get(url)\n",
        "\n",
        "    #         if response.status_code == 200:\n",
        "    #             result = response.json()\n",
        "\n",
        "    #             # Extract email pattern\n",
        "    #             pattern = result.get(\"data\", {}).get(\"pattern\", \"\")\n",
        "    #             if pattern:\n",
        "    #                 company_data[\"email_pattern\"] = pattern\n",
        "\n",
        "    #             # Extract potential contacts\n",
        "    #             contacts = result.get(\"data\", {}).get(\"emails\", [])\n",
        "    #             potential_contacts = []\n",
        "\n",
        "    #             for contact in contacts:\n",
        "    #                 position = contact.get(\"position\", \"\").lower()\n",
        "    #                 # Focus on decision makers\n",
        "    #                 if any(role in position for role in [\"vp\", \"vice president\", \"director\", \"manager\", \"head\", \"chief\", \"ceo\", \"cto\", \"cio\", \"founder\"]):\n",
        "    #                     potential_contacts.append({\n",
        "    #                         \"name\": f\"{contact.get('first_name', '')} {contact.get('last_name', '')}\",\n",
        "    #                         \"position\": contact.get(\"position\", \"\"),\n",
        "    #                         \"email\": contact.get(\"value\", \"\"),\n",
        "    #                         \"confidence_score\": contact.get(\"confidence\", 0)\n",
        "    #                     })\n",
        "\n",
        "    #             if potential_contacts:\n",
        "    #                 company_data[\"contacts\"] = potential_contacts\n",
        "\n",
        "    #             logger.info(\"Successfully enriched data with Hunter.io API\")\n",
        "    #         else:\n",
        "    #             logger.warning(f\"Error from Hunter.io API: {response.status_code} - {response.text}\")\n",
        "\n",
        "    #     except Exception as e:\n",
        "    #         logger.error(f\"Error enriching with Hunter.io: {str(e)}\")\n",
        "\n",
        "    #     return company_data\n",
        "\n",
        "    def scrape_company_website(self, company_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Scrape company website for additional information.\n",
        "        \"\"\"\n",
        "        website = company_data.get(\"website\", \"\")\n",
        "        if not website:\n",
        "            logger.warning(\"No website available for scraping\")\n",
        "            return company_data\n",
        "\n",
        "        try:\n",
        "            response = requests.get(website, headers=self.headers, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # Try to extract company description if not already available\n",
        "                if not company_data.get(\"description\"):\n",
        "                    meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "                    if meta_desc:\n",
        "                        company_data[\"description\"] = meta_desc.get(\"content\", \"\")\n",
        "\n",
        "                # Look for about/team pages\n",
        "                about_links = []\n",
        "                for link in soup.find_all(\"a\", href=True):\n",
        "                    href = link.get(\"href\")\n",
        "                    text = link.text.lower()\n",
        "                    if \"about\" in text or \"team\" in text or \"leadership\" in text or \"management\" in text:\n",
        "                        # Make relative URLs absolute\n",
        "                        if href.startswith(\"/\"):\n",
        "                            href = f\"{website.rstrip('/')}{href}\"\n",
        "                        elif not href.startswith((\"http://\", \"https://\")):\n",
        "                            href = f\"{website.rstrip('/')}/{href.lstrip('/')}\"\n",
        "                        about_links.append(href)\n",
        "\n",
        "                # Store links for further processing\n",
        "                if about_links:\n",
        "                    company_data[\"about_pages\"] = list(set(about_links))\n",
        "\n",
        "                    # Try to extract team information from the first about page\n",
        "                    if about_links[0]:\n",
        "                        try:\n",
        "                            about_response = requests.get(about_links[0], headers=self.headers, timeout=10)\n",
        "                            if about_response.status_code == 200:\n",
        "                                about_soup = BeautifulSoup(about_response.text, 'html.parser')\n",
        "\n",
        "                                # Look for team members (this is a simple approach, may need refinement)\n",
        "                                team_members = []\n",
        "                                for tag in about_soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"strong\", \"b\"]):\n",
        "                                    if tag.find_next_sibling() and tag.find_next_sibling().name in [\"p\", \"div\"]:\n",
        "                                        name = tag.text.strip()\n",
        "                                        position = tag.find_next_sibling().text.strip()\n",
        "\n",
        "                                        # Filter for decision makers\n",
        "                                        position_lower = position.lower()\n",
        "                                        if any(role in position_lower for role in [\"vp\", \"vice president\", \"director\", \"manager\", \"head\", \"chief\", \"ceo\", \"cto\", \"cio\", \"founder\", \"president\"]):\n",
        "                                            team_members.append({\n",
        "                                                \"name\": name,\n",
        "                                                \"position\": position\n",
        "                                            })\n",
        "\n",
        "                                if team_members:\n",
        "                                    company_data.setdefault(\"team_members\", []).extend(team_members)\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logger.error(f\"Error scraping about page: {str(e)}\")\n",
        "\n",
        "                logger.info(\"Successfully scraped company website\")\n",
        "            else:\n",
        "                logger.warning(f\"Error scraping website: {response.status_code}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error scraping company website: {str(e)}\")\n",
        "\n",
        "        return company_data\n",
        "\n",
        "    def enrich_with_serp(self, company_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Use SERP API to get additional information about the company.\n",
        "        \"\"\"\n",
        "        if not self.serp_api_key:\n",
        "            logger.warning(\"SERP API key not available. Skipping enrichment.\")\n",
        "            return company_data\n",
        "\n",
        "        try:\n",
        "            company_name = company_data.get(\"company_name\", \"\")\n",
        "            if not company_name:\n",
        "                logger.warning(\"No company name available for SERP enrichment\")\n",
        "                return company_data\n",
        "\n",
        "            # Prepare search query\n",
        "            query = f\"{company_name} company revenue employees\"\n",
        "\n",
        "            # Query SERP API\n",
        "            url = \"https://serpapi.com/search\"\n",
        "            params = {\n",
        "                \"engine\": \"google\",\n",
        "                \"q\": query,\n",
        "                \"api_key\": self.serp_api_key\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, params=params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "\n",
        "                # Extract organic results\n",
        "                organic_results = result.get(\"organic_results\", [])\n",
        "\n",
        "                # Extract knowledge graph\n",
        "                knowledge_graph = result.get(\"knowledge_graph\", {})\n",
        "\n",
        "                # Update company data with information from knowledge graph\n",
        "                if knowledge_graph:\n",
        "                    company_data[\"industry\"] = knowledge_graph.get(\"category\", company_data.get(\"industry\", \"\"))\n",
        "\n",
        "                    # Try to extract employee count or revenue from description\n",
        "                    description = knowledge_graph.get(\"description\", \"\")\n",
        "\n",
        "                    # Look for employee count pattern\n",
        "                    employee_match = re.search(r'(\\d[\\d,]+) employees', description)\n",
        "                    if employee_match and not company_data.get(\"employees\"):\n",
        "                        company_data[\"employees\"] = employee_match.group(1)\n",
        "\n",
        "                    # Look for revenue pattern\n",
        "                    revenue_match = re.search(r'\\$([\\d\\.]+) (million|billion|trillion)', description)\n",
        "                    if revenue_match and not company_data.get(\"revenue\"):\n",
        "                        amount = float(revenue_match.group(1))\n",
        "                        unit = revenue_match.group(2)\n",
        "\n",
        "                        if unit == \"billion\":\n",
        "                            amount *= 1000\n",
        "                        elif unit == \"trillion\":\n",
        "                            amount *= 1000000\n",
        "\n",
        "                        company_data[\"revenue\"] = f\"${amount} million\"\n",
        "\n",
        "                logger.info(\"Successfully enriched data with SERP API\")\n",
        "            else:\n",
        "                logger.warning(f\"Error from SERP API: {response.status_code} - {response.text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error enriching with SERP: {str(e)}\")\n",
        "\n",
        "        return company_data\n",
        "\n",
        "    def find_decision_makers(self, company_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Combine all sources to identify decision makers.\n",
        "        \"\"\"\n",
        "        decision_makers = []\n",
        "\n",
        "        # Collect from all sources\n",
        "        if \"contacts\" in company_data:\n",
        "            for contact in company_data[\"contacts\"]:\n",
        "                decision_makers.append({\n",
        "                    \"name\": contact.get(\"name\", \"\"),\n",
        "                    \"position\": contact.get(\"position\", \"\"),\n",
        "                    \"email\": contact.get(\"email\", \"\"),\n",
        "                    \"source\": \"hunter.io\"\n",
        "                })\n",
        "\n",
        "        if \"team_members\" in company_data:\n",
        "            for member in company_data[\"team_members\"]:\n",
        "                # Check if this person is already in our list\n",
        "                if not any(dm[\"name\"].lower() == member[\"name\"].lower() for dm in decision_makers):\n",
        "                    decision_makers.append({\n",
        "                        \"name\": member.get(\"name\", \"\"),\n",
        "                        \"position\": member.get(\"position\", \"\"),\n",
        "                        \"source\": \"website\"\n",
        "                    })\n",
        "\n",
        "        if \"decision_makers\" in company_data and isinstance(company_data[\"decision_makers\"], list):\n",
        "            for dm in company_data[\"decision_makers\"]:\n",
        "                # Handle different formats\n",
        "                if isinstance(dm, dict):\n",
        "                    name = dm.get(\"name\", \"\")\n",
        "                    position = dm.get(\"position\", \"\")\n",
        "                elif isinstance(dm, str):\n",
        "                    parts = dm.split(\",\", 1)\n",
        "                    name = parts[0].strip()\n",
        "                    position = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                # Check if this person is already in our list\n",
        "                if not any(existing_dm[\"name\"].lower() == name.lower() for existing_dm in decision_makers):\n",
        "                    decision_makers.append({\n",
        "                
        "        # Filter for relevant positions\n",
        "        relevant_keywords = [\n",
        "            \"vp\", \"vice president\", \"director\", \"head\", \"chief\",\n",
        "            \"ceo\", \"cto\", \"cio\", \"founder\", \"president\", \"product\",\n",
        "            \"development\", \"innovation\", \"r&d\", \"research\", \"coating\"\n",
        "        ]\n",
        "\n",
        "        filtered_decision_makers = []\n",
        "        for dm in decision_makers:\n",
        "            position = dm.get(\"position\", \"\").lower()\n",
        "            if any(keyword in position for keyword in relevant_keywords):\n",
        "                filtered_decision_makers.append(dm)\n",
        "\n",
        "        # Update company data\n",
        "        company_data[\"key_decision_makers\"] = filtered_decision_makers\n",
        "\n",
        "        return company_data\n",
        "\n",
        "    def enrich_lead(self, raw_data: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Main method to enrich a sales lead with additional information.\n",
        "        \"\"\"\n",
        "        # Step 1: Parse input data\n",
        "        company_data = self.parse_input_data(raw_data)\n",
        "        logger.info(f\"Parsed initial company data: {json.dumps(company_data, indent=2)}\")\n",
        "\n",
        "    
        "\n",
        "        # Step 3: Enrich with Clearbit (if domain is available)\n",
        "        # company_data = self.enrich_with_clearbit(company_data)\n",
        "\n",
        "        # Step 4: Scrape company website for additional information\n",
        "        company_data = self.scrape_company_website(company_data)\n",
        "\n",
        "        # Step 5: Enrich with Hunter.io for contact information\n",
        "        # company_data = self.enrich_with_hunter(company_data)\n",
        "\n",
        "        # Step 6: Use SERP API for additional context (optional)\n",
        "        company_data = self.enrich_with_serp(company_data)\n",
        "\n",
        "        # Step 7: Find decision makers\n",
        "        company_data = self.find_decision_makers(company_data)\n",
        "\n",
        "        return company_data\n",
        "\n"
      ],
      "metadata": {
        "id": "L30zSDSrK-h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Example usage\n",
        "    agent = SalesLeadEnrichmentAgent()\n",
        "\n",
        "    # Sample input data (replace with actual input)\n",
        "    sample_data = \"\"\"\n",
        "    **Company Name**\n",
        "    **Profile URL**\n",
        "    **Description**\n",
        "    **Website**\n",
        "    **Phone**\n",
        "    **Email**\n",
        "    **Address**\n",
        "    **Categories**\n",
        "    **Social Media**\n",
        "    **Almaas Aldahabi Plastic Industries for P.V.C Foam board production L.L.C**\n",
        "    https://isasignexpo2025.mapyourshow.com/8_0/exhibitor/exhibitor-details.cfm?exhid=5162\n",
        "\n",
        "    https://www.instagram.com/alalmaas_group/\n",
        "\n",
        "\n",
        "    Al Ibrahimia near Karbala ChannelKarbala56001Iraq\n",
        "\n",
        "    https://web.facebook.com/photo/?fbid=183943094254842&set=a.183943087588176, https://www.instagram.com/alalmaas_group/\n",
        "    \"\"\"\n",
        "\n",
        "    # Enrich the lead\n",
        "    enriched_data = agent.enrich_lead(sample_data)\n",
        "\n",
        "    # Print results\n",
        "    print(json.dumps(enriched_data, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLR_b82rNDZl",
        "outputId": "9cbf3a1f-5fe7-4d9a-8b77-369dd00279d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Firecrawl API key not found. Some features may be limited.\n",
            "WARNING:__main__:No website available for scraping\n",
            "WARNING:__main__:SERP API key not available. Skipping enrichment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"company_name\": \"**Profile URL**\",\n",
            "  \"description\": \"**Website**\",\n",
            "  \"phone\": \"**Email**\",\n",
            "  \"email\": \"**Address**\",\n",
            "  \"address\": \"**Categories**\",\n",
            "  \"categories\": \"**Social Media**\",\n",
            "  \"social_media\": [\n",
            "    \"https://www.instagram.com/alalmaas_group/\",\n",
            "    \"https://www.instagram.com/alalmaas_group/\"\n",
            "  ],\n",
            "  \"domain\": \"\",\n",
            "  \"key_decision_makers\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EA - 2"
      ],
      "metadata": {
        "id": "S3y9vMulQDu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai serpapi beautifulsoup4 requests pandas tldextract\n",
        "!pip install google-search-results --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5TLMBBY1QIA-",
        "outputId": "f01d00d8-8ef9-4556-cc87-1904d1fa0558"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.77.0)\n",
            "Requirement already satisfied: serpapi in /usr/local/lib/python3.11/dist-packages (0.1.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (5.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.18.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.11/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "from serpapi import GoogleSearch\n",
        "from bs4 import BeautifulSoup\n",
        "import openai\n",
        "import tldextract\n",
        "import time"
      ],
      "metadata": {
        "id": "q8Lxx0AnQGYp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# CONFIGURATION\n",
        "# -------------------\n",
        "SERP_API_KEY = \"xyz\"  # from serpapi.com\n",
        "# OPENAI_API_KEY = \"your_openai_api_key\"  # for LLM enrichment\n",
        "\n",
        "# openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# -------------------\n",
        "# Load companies\n",
        "# -------------------\n",
        "df = pd.read_csv('/content/ISA_companies_20250513.csv')  # must have 'company_name' column\n",
        "df['company_website'] = ''\n",
        "df['company_description'] = ''\n",
        "df['employee_size'] = ''\n",
        "df['revenue_estimate'] = ''\n",
        "\n",
        "# -------------------\n",
        "# Get Company Website via Google SERP API\n",
        "# -------------------\n",
        "def get_company_website(company_name):\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": f\"{company_name} official site\",\n",
        "        \"api_key\": SERP_API_KEY\n",
        "    }\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "    if 'organic_results' in results:\n",
        "        for res in results['organic_results']:\n",
        "            link = res.get('link', '')\n",
        "            if 'linkedin.com' in link:\n",
        "                continue\n",
        "            return link\n",
        "    return ''\n",
        "\n",
        "# -------------------\n",
        "# Scrape Metadata from Website or Use Snippet\n",
        "# -------------------\n",
        "def scrape_website_description(url):\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        resp = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "        desc = soup.find('meta', attrs={'name': 'description'})\n",
        "        if desc:\n",
        "            return desc['content']\n",
        "        p_tag = soup.find('p')\n",
        "        return p_tag.text if p_tag else ''\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "# -------------------\n",
        "# LLM Cleanup - not in this version\n",
        "# -------------------\n",
        "# def clean_with_llm(text, company_name):\n",
        "#     try:\n",
        "#         prompt = f\"Given this text about {company_name}, write a clean and concise company description:\\n\\n{text}\"\n",
        "#         response = openai.ChatCompletion.create(\n",
        "#             model=\"gpt-4\",\n",
        "#             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "#             temperature=0.5,\n",
        "#             max_tokens=150\n",
        "#         )\n",
        "#         return response['choices'][0]['message']['content'].strip()\n",
        "#     except:\n",
        "        return text\n",
        "\n",
        "# -------------------\n",
        "# Main Loop\n",
        "# -------------------\n",
        "for idx, row in df.iterrows():\n",
        "    company = row['Company Name']\n",
        "    print(f\"Processing: {company}\")\n",
        "\n",
        "    # Step 1\n",
        "    website = get_company_website(company)\n",
        "    df.at[idx, 'company_website'] = website\n",
        "    if website == '':\n",
        "        continue\n",
        "\n",
        "    # Step 2\n",
        "    raw_desc = scrape_website_description(website)\n",
        "    # Step 3\n",
        "    # cleaned_desc = clean_with_llm(raw_desc, company)\n",
        "    # df.at[idx, 'company_description'] = cleaned_desc\n",
        "\n",
        "    # Step 4 (try LinkedIn fallback)\n",
        "    if 'linkedin.com' not in website:\n",
        "        domain = tldextract.extract(website).registered_domain\n",
        "        df.at[idx, 'employee_size'] = f\"Try LinkedIn for: {domain}\"\n",
        "        df.at[idx, 'revenue_estimate'] = f\"Try Crunchbase/Zoominfo for: {domain}\"\n",
        "\n",
        "    time.sleep(3)  # Respect rate limits\n",
        "\n",
        "# -------------------\n",
        "# Save enriched file\n",
        "# -------------------\n",
        "df.to_csv('/content/enriched_companies.csv', index=False)\n",
        "print(\"Enrichment complete! File saved.\")\n"
      ],
      "metadata": {
        "id": "ZWnwQ_JBQFQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4ed119-f036-40ed-f2af-33f574c2e9e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: Almaas Aldahabi Plastic Industries for P.V.C Foam board production L.L.C\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-507b05bb8dcb>:90: DeprecationWarning: The 'registered_domain' property is deprecated and will be removed in the next major version. Use 'top_domain_under_public_suffix' instead, which has the same behavior but a more accurate name.\n",
            "  domain = tldextract.extract(website).registered_domain\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: AP Lazer | Laser Cutters + Engravers\n",
            "Processing: CUTWORX USA\n",
            "Processing: Elliott Equipment Company\n",
            "Processing: Epilog Laser\n",
            "Processing: General Formulations\n",
            "Processing: Gildo Profilati\n",
            "Processing: Hirsch Solutions, LLC\n",
            "Processing: InfoFlo Print\n",
            "Processing: Laguna Tools Inc.\n",
            "Processing: Lintec of America, Inc.\n",
            "Processing: Magnum Magnetics Corporation\n",
            "Processing: Murdoch Engineering\n",
            "Processing: Permits by Mirza Corp. dba Permit Ninjas\n",
            "Processing: Photo Tex Group Inc.\n",
            "Processing: Principal Industries\n",
            "Processing: SDS Automation\n",
            "Processing: Signage Details\n",
            "Processing: StratoJet\n",
            "Processing: Stud Welding Products Inc.\n",
            "Processing: Trotec Laser\n",
            "Processing: Vibrant Print Solutions\n",
            "Processing: Vytek (Vinyl Technologies)\n",
            "Processing: X-Edge Products, Inc\n",
            "Processing: Zund America\n",
            "Processing: 1 Source BPO\n",
            "Processing: 3A Composites USA, Inc.\n",
            "Processing: 3M Commercial Solutions\n",
            "Processing: 4Ever Products\n",
            "Processing: 4over LLC\n",
            "Processing: 4U Media & Displays, LLC\n",
            "Processing: 7C Digital Printing Textile Co.,Ltd\n",
            "Processing: A&C Plastics\n",
            "Processing: A.R.K. Ramos Foundry & Mfg. Co.\n",
            "Processing: AA ACRYLIC CO.，LTD\n",
            "Processing: AA LED Supply Corp.\n",
            "Processing: ABC Sign Products\n",
            "Processing: Abitech\n",
            "Processing: ABLE\n",
            "Processing: Action Lighting\n",
            "Processing: ADHES PACKAGING TECHNOLOGY (MALAYSIA) SDN.BHD.\n",
            "Processing: ADMAX Exhibit & Display Ltd.\n",
            "Processing: Advanced Film Technology\n",
            "Processing: Advanced Greig Laminators, Inc.\n",
            "Processing: Advanced Modern Sign\n",
            "Processing: Advantage Innovations, Inc\n",
            "Processing: Aeon Laser\n",
            "Processing: Agfa Corporation\n",
            "Processing: AgiLight\n",
            "Processing: AkzoNobel\n",
            "Processing: Alfa(Wuxi) Industrial Co.,Ltd\n",
            "Processing: Allanson International\n",
            "Processing: Alliance Technology Corp.  & SCG DIGITAL AMERICA INC.\n",
            "Processing: ALLSIGN INTERNATIONAL TECHNOLOGY CO.,LTD\n",
            "Processing: Alro Steel & Plastics\n",
            "Processing: Altec Industries\n",
            "Processing: Aludecor\n",
            "Processing: Alumanate ACM\n",
            "Processing: American Ground Screw, Inc.\n",
            "Processing: Anhui Angran Green Technology Co.,Ltd.\n",
            "Processing: Anhui sunsign AD co., ltd\n",
            "Processing: AnyLux Co, Ltd\n",
            "Processing: AOMYA DIGITAL TECHNOLOGY CO., LTD.\n",
            "Processing: Arcus Printers\n",
            "Processing: Arizona Sign Association\n",
            "Processing: Arlon Graphics\n",
            "Processing: Art Sign Works\n",
            "Processing: Arter Neon Sign Inc\n",
            "Processing: Ascent Equipment\n",
            "Processing: Ascentium Capital\n",
            "Processing: Aurora Science and Technology Co., Ltd\n",
            "Processing: Avery Dennison Graphics Solutions\n",
            "Processing: Awning and Sign Contractors\n",
            "Enrichment complete! File saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from serpapi import GoogleSearch\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "wXbKWQyqTxcT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## additional layer of enrichment\n",
        "\n",
        "def search_linkedin_company_info(company_name):\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": f\"{company_name} site:linkedin.com/company\",\n",
        "        \"api_key\": SERP_API_KEY\n",
        "    }\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "\n",
        "    for res in results.get(\"organic_results\", []):\n",
        "        link = res.get(\"link\", \"\")\n",
        "        if \"linkedin.com/company\" in link:\n",
        "            snippet = res.get(\"snippet\", \"\")\n",
        "            return link, snippet\n",
        "    return None, None\n",
        "\n",
        "def search_linkedin_people(company_name, role_keywords=[\"VP\", \"Director\", \"Innovation\", \"Product\", \"R&D\"]):\n",
        "    query = f\"{company_name} \" + \" OR \".join(role_keywords) + \" site:linkedin.com/in\"\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERP_API_KEY\n",
        "    }\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "\n",
        "    contacts = []\n",
        "    for res in results.get(\"organic_results\", []):\n",
        "        title = res.get(\"title\", \"\")\n",
        "        link = res.get(\"link\", \"\")\n",
        "        if any(role.lower() in title.lower() for role in role_keywords):\n",
        "            contacts.append(f\"{title} | {link}\")\n",
        "    return contacts\n",
        "\n",
        "# dataframe\n",
        "df = pd.read_csv('/content/enriching_agent_tester_set.csv')\n",
        "\n",
        "# add columns\n",
        "df['linkedin_company'] = ''\n",
        "df['linkedin_snippet'] = ''\n",
        "df['decision_makers'] = ''\n",
        "\n",
        "# Enrich via LinkedIn\n",
        "for idx, row in df.iterrows():\n",
        "    name = row['Company Name']\n",
        "    print(f\"Searching LinkedIn for: {name}\")\n",
        "\n",
        "    comp_url, comp_snippet = search_linkedin_company_info(name)\n",
        "    people = search_linkedin_people(name)\n",
        "\n",
        "    df.at[idx, 'linkedin_company'] = comp_url\n",
        "    df.at[idx, 'linkedin_snippet'] = comp_snippet\n",
        "    df.at[idx, 'decision_makers'] = \"; \".join(people)\n",
        "\n",
        "    time.sleep(3)  # avoid rate limits\n",
        "\n",
        "df.to_csv('/content/enriched_v2_companies.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YzlLUWGTt-B",
        "outputId": "39d989b2-3e57-487c-874c-14d60765a171"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching LinkedIn for: Almaas Aldahabi Plastic Industries for P.V.C Foam board production L.L.C\n",
            "Searching LinkedIn for: AP Lazer | Laser Cutters + Engravers\n",
            "Searching LinkedIn for: CUTWORX USA\n",
            "Searching LinkedIn for: Elliott Equipment Company\n",
            "Searching LinkedIn for: Epilog Laser\n",
            "Searching LinkedIn for: General Formulations\n"
          ]
        }
      ]
    }
  ]
}
